{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import urllib\n",
    "import pandas as pd    \n",
    "    \n",
    "companies=[] #insert the names of companies whose headquarters location you wish to find     \n",
    "    \n",
    "\n",
    "#you can test it with 30 random companies from the S&P500 (credit for the dataset: rufuspollock)\n",
    "\n",
    "#import random\n",
    "#df=pd.read_csv(\"https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv\") \n",
    "#companies=df['Name'].tolist()\n",
    "#companies=random.sample(companies,30)\n",
    "#companies[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we create an empty list for storing locations\n",
    "\n",
    "locations=[] \n",
    "\n",
    "# we use a for loop to loop over each company name in our list\n",
    "\n",
    "for item in companies:\n",
    "    try: #skip to the next item if DisambiguationError\n",
    "        page=wikipedia.page(wikipedia.search(item)[0]) #this returns the first page in the search results list\n",
    "        url=page.url #returns the url of the page\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(page) #creates parse tree to extract data from html\n",
    "        infobox=soup.find(class_=\"infobox vcard\")\n",
    "        if infobox: # script is designed only for pages with an infobox; we skip to the next item if absent\n",
    "            hq=infobox.find('th',string='Headquarters') #finds the \"th\" element containing string \"Headquarters\" if there is\n",
    "            if hq:\n",
    "                sib=hq.find_next_siblings('td')\n",
    "                locations.append(sib[0].text.replace('\\n','')) #we only want the text; we get rid of the '\\n's\n",
    "            else: locations.append('')\n",
    "        else: locations.append('')\n",
    "    except:locations.append('') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we create a pandas df from our 2 lists\n",
    "\n",
    "headquarters_df=pd.DataFrame(data={\"firm\": companies, 'headquarted':locations})\n",
    "headquarters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this returns the percentage of companies for which a location was found\n",
    "\n",
    "success_rate=sum(1 for item in locations if item!=\"\")/len(locations)\n",
    "success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can export your df to csv\n",
    "\n",
    "headquarters_df.to_csv('myfile.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
